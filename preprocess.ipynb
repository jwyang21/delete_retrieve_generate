{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from random import sample\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "random.seed(2022)\n",
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python langdetect package: https://pypi.org/project/langdetect/\n",
    "- Ïù¥ Ìå®ÌÇ§ÏßÄÏùò input Î¨∏Ïû• ÏòÅÏñ¥ Ïó¨Î∂Ä Î∂ÑÎ•ò Í≤∞Í≥ºÍ∞Ä 100% Ï†ïÌôïÌïòÏßÑ ÏïäÏßÄÎßå, Ïù¥ Ìå®ÌÇ§ÏßÄÍ∞Ä ÏòÅÏñ¥Í∞Ä ÏïÑÎãàÎùºÍ≥† ÌåêÎ≥ÑÌïú Î¨∏Ïû•Îì§ÏùÑ Îã§ Î∫êÏùÑ ÎïåÎèÑ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞Í∞Ä Ï∂©Î∂ÑÌûà Ïª§ÏÑú Ï†ÑÏ≤òÎ¶¨ Í≥ºÏ†ïÏóê Ìè¨Ìï®ÏãúÌÇ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sampling small trainset, use random.sample()\n",
    "- Python‚Äôs random module provides a sample() function for random sampling, randomly picking more than one element from the list without repeating elements.            \n",
    "- referred to https://pynative.com/python-random-sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savedir = '/data/project/jeewon/coursework/2022-2/nlp/data/processed'\n",
    "#datadir = '/data/project/jeewon/coursework/2022-2/nlp/data/'\n",
    "datadir = './data/raw'\n",
    "savedir = './data/processed'\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = ['0', '1', '2','3','4','5','6','7','8','9',]\n",
    "marks= ['¬©','*', '..................','- - - - - - - - - - - - - -', 'CÃ®ÃºÃ±eÃµÕ°ÕÄÕöÃ¨ÕñÃ†ÃúrÕüÃ®Õ°ÕöÕÖÃúÃñÃ•ÃóÃ•vÃ©ÃºeÕÖÕâÃñÃ≠ÃôÃ≥ÃóÃ±ÕñlÕò“âÃóÃ§Ã†ÕÖÕñoÃ•ÃñÕçÕçÃü', 'üòÇ', 'üî•','üëâ', 'vÃ©ÃºeÕÖÕâÃñÃ≠ÃôÃ≥ÃóÃ±ÕñlÕò“âÃóÃ§Ã†ÕÖÕñoÃ•ÃñÕçÕçÃü', 'üê∂','üêï','üê©','üêÖ','üêÜ','üêæ','üå∑','‚ù§','üíô','üíö','üíõ','‚ù§']\n",
    "unavailable_strings = ['copyright',  'published by']  \n",
    "en_strings = 'a b c d e f g h i j k l m n o p q r s t u v w x y z'.split(' ')\n",
    "small_trainset_size = 221630\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_unavailables(feature, poem_flag, df): \n",
    "    \n",
    "    print(\"input df shape: \", df.shape)\n",
    "    #feature: numbers, marks, unavailable_strings\n",
    "    \n",
    "    data_ = 'poem' if poem_flag==True else 'reddit'\n",
    "    \n",
    "    globals()[data_+'_'+feature+'_indices'] = []\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        current_string = str(df.text.values[i]).lower()\n",
    "        #if detect(current_string) != 'en':\n",
    "        for k in globals()[feature]:\n",
    "            if k in current_string and i not in globals()[data_+'_'+feature+'_indices']:\n",
    "                globals()[data_+'_'+feature+'_indices'].append(i)\n",
    "\n",
    "    print(\"num_excluded: \", len(globals()[data_+'_'+feature+'_indices']))\n",
    "    \n",
    "    if df.iloc[globals()[data_+'_'+feature+'_indices'],:].shape[0] >= 10:\n",
    "        print(\"example sentences that are excluded: \")\n",
    "        print(df.iloc[globals()[data_+'_'+feature+'_indices'],:].sample(10).text.values)\n",
    "    else:\n",
    "        print(\"example sentences that are excluded: \")\n",
    "        print(df.iloc[globals()[data_+'_'+feature+'_indices'],:].text.values)\n",
    "\n",
    "    df.drop(globals()[data_+'_'+feature+'_indices'], axis = 0, inplace = True)\n",
    "    print(\"data shape after exclusion: \", df.shape)\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, s_flag, savedir, train_fname, test_fname, small_train_fname=''):\n",
    "    #print(df.shape)\n",
    "    test_ind = np.random.randint(low=0, high=df.shape[0], size=test_size)\n",
    "    train_ind = np.delete(np.arange(df.shape[0]), test_ind)\n",
    "    #print(len(test_ind))\n",
    "    #print(len(train_ind))\n",
    "    if len(test_ind)+len(train_ind) - df.shape[0]!= 0:\n",
    "        raise ValueError\n",
    "    #print(\"train: {}, test: {}\".format(len(train_ind), len(test_ind)))\n",
    "    \n",
    "    train = df.loc[train_ind].copy()\n",
    "    test = df.loc[test_ind].copy()\n",
    "    #train.index = np.arange(train.shape[0])\n",
    "    #test.index = np.arange(test.shape[0])\n",
    "    \n",
    "    print(\"train set size: \", train.shape)\n",
    "    print(\"test set size: \", test.shape)\n",
    "        \n",
    "    # save train/test sets\n",
    "    reddit_train.to_csv(os.path.join(savedir, train_fname), index = False)\n",
    "    reddit_test.to_csv(os.path.join(savedir, test_fname), index = False)\n",
    "    \n",
    "    # (optional) make small trainset\n",
    "    if s_flag==True:\n",
    "        train.index = np.arange(train.shape[0])\n",
    "        small_train_ind = sample(np.arange(train.shape[0]).tolist(), small_trainset_size)\n",
    "        small_trainset = train.iloc[small_train_ind,:].copy()\n",
    "        small_reddit_trainset.to_csv(os.path.join(savedir, small_train_fname), index = False)\n",
    "        \n",
    "    return train, test, small_trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly sample 50 sentences for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = pd.read_csv(os.path.join(datadir, 'poem1_15.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(os.path.join(datadir, 'reddit_15.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559527, 2)\n",
      "(659187, 2)\n"
     ]
    }
   ],
   "source": [
    "print(poem.shape)\n",
    "print(reddit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect unavailables in poem and reddit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. exclude not-containing-language sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3161\n",
      "['‰∫âËÆ§ÊÖàÊÅ©Á¥´Áâ°‰∏π' '90' '31' '45' '*' 'ÂøÉÁõ∏Âç∞ÔºåÊ≥™È¢ëÊΩ∏' '‡§µ‡§ø‡§∑ ‡§µ‡§Æ‡§® ‡§ï‡§∞‡•á‡§Ç' '3' '.'\n",
      " '–ü–æ–∫–∞ —É–¥–µ–ª –∏—Ö –≤–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ']\n",
      "(556366, 2)\n"
     ]
    }
   ],
   "source": [
    "no_words_sentence_indices = []\n",
    "for i in range(poem.shape[0]):\n",
    "    current_string = str(poem.text.values[i]).lower()\n",
    "    counter_dictionary = Counter(current_string)\n",
    "    num_en_str = 0\n",
    "    for en_str in en_strings:\n",
    "        if en_str in list(counter_dictionary.keys()):\n",
    "            num_en_str += counter_dictionary[en_str]\n",
    "    if num_en_str == 0 and i not in no_words_sentence_indices:\n",
    "        no_words_sentence_indices.append(i)\n",
    "print(len(no_words_sentence_indices))\n",
    "print(poem.iloc[no_words_sentence_indices,:].sample(10).text.values)\n",
    "poem_exclude_no_words = poem.drop(no_words_sentence_indices, axis=0).copy()\n",
    "poem_exclude_no_words.index = np.arange(poem_exclude_no_words.shape[0])\n",
    "print(poem_exclude_no_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. exclude sentences with num_words <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64811\n",
      "['Comes to life' 'some watch' '(inhale)' 'my  eyes' '(Copyright ¬©05/2010)'\n",
      " 'This sweet May-morning,' 'Of the beloved,' '‚ÄòRomance Point‚Äô.'\n",
      " 'she walked away' 'The aches...the pains...']\n",
      "(491555, 2)\n"
     ]
    }
   ],
   "source": [
    "few_words_sentence_indices = []\n",
    "for i in range(poem_exclude_no_words.shape[0]):\n",
    "    current_string = str(poem_exclude_no_words.text.values[i]).lower()\n",
    "    if len(current_string.split(' ')) <= 3 and i not in few_words_sentence_indices:\n",
    "        few_words_sentence_indices.append(i)\n",
    "print(len(few_words_sentence_indices))\n",
    "print(poem_exclude_no_words.iloc[few_words_sentence_indices,:].sample(10).text.values)\n",
    "poem_exclude_few_words = poem_exclude_no_words.drop(few_words_sentence_indices, axis=0).copy()\n",
    "poem_exclude_few_words.index = np.arange(poem_exclude_few_words.shape[0])\n",
    "print(poem_exclude_few_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. exclude numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape:  (491555, 2)\n",
      "num_excluded:  5451\n",
      "example sentences that are excluded: \n",
      "['Parody William Carlos Williams 1883_1963  The Red Wheelbarrow'\n",
      " '1  Has it a body?  2  Ay, and wings,'\n",
      " \"8     Whilst the landscape's odours rise,\"\n",
      " '16 Where even the little brambles would not yield,'\n",
      " 'Copyright ¬© Muzahidul Reza | 10/02/2016'\n",
      " 'Just written media August 2011.'\n",
      " 'II 3. History of World Philosophy by Will Duran' '6 lines poem ‚îÄ'\n",
      " '16   And her torn fan gives real signs of woe.'\n",
      " \"Verse Paragraph For Tina's Boundary - 1\"]\n",
      "data shape after exclusion:  (486104, 2)\n"
     ]
    }
   ],
   "source": [
    "poem_exclude_number = exclude_unavailables(feature='numbers', poem_flag=True, df=poem_exclude_few_words.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. exclude marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape:  (486104, 2)\n",
      "num_excluded:  402\n",
      "example sentences that are excluded: \n",
      "['*inspired by A S Neil' 'trapped in your f*cked up world'\n",
      " \"Walloped *em and whacked 'em.\" 'shot in the ****'\n",
      " '................................. thine every part'\n",
      " '*Mr. Tyre is her favorite toy, a present from the one who feeds her.'\n",
      " \"Don't forget kiss kiss kiss with your loyalty*\"\n",
      " 'Afterwards kiss kiss kiss my thoughts*'\n",
      " '.................................... oh mine heart?'\n",
      " '(free verse) * The Doings Of McPherson']\n",
      "data shape after exclusion:  (485702, 2)\n"
     ]
    }
   ],
   "source": [
    "poem_exclude_mark = exclude_unavailables(feature='marks', poem_flag=True, df=poem_exclude_number.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. exclude not-english-sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32763\n",
      "['Nor make too long a stay;' 'Golden ducks are swimming in joy,'\n",
      " 'So make a enrol' 'Muffled giggle kids gleam' 'Like Keats and Blake'\n",
      " \"Unusual I don't sleep really\" 'forsvant de alle; barnet kun forble der'\n",
      " 'Black milk of daybreak we drink you at night' 'Staring at my feet.'\n",
      " 'Poor Damon too, was sunk in deadly grief,']\n",
      "(452939, 2)\n"
     ]
    }
   ],
   "source": [
    "not_en_indices = []\n",
    "for i in range(poem_exclude_mark.shape[0]):\n",
    "    current_string = str(poem_exclude_mark.text.values[i]).lower()\n",
    "    if detect(current_string) != 'en' and i not in not_en_indices:\n",
    "        not_en_indices.append(i)\n",
    "print(len(not_en_indices))\n",
    "print(poem_exclude_mark.iloc[not_en_indices,:].sample(10).text.values)\n",
    "poem_exclude_not_en = poem_exclude_mark.drop(not_en_indices, axis=0).copy()\n",
    "poem_exclude_not_en.index = np.arange(poem_exclude_not_en.shape[0])\n",
    "print(poem_exclude_not_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. exclude 'copyright' and 'published by'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape:  (452939, 2)\n",
      "num_excluded:  26\n",
      "example sentences that are excluded: \n",
      "['Copyright reserved by author' 'written by salma torrez. copyright'\n",
      " 'My poems are copyright. I am sharing my'\n",
      " 'Copyright reserved by the Author' 'Copyright reserved by the Author'\n",
      " 'wore their ancient copyrights like new maidenheads.'\n",
      " 'Copyright reserved by the author' 'Copyright reserved by the Author'\n",
      " 'Copyright reserved by author' 'Copyright reserved by the Author']\n",
      "data shape after exclusion:  (452913, 2)\n"
     ]
    }
   ],
   "source": [
    "poem_exclude_unavailables = exclude_unavailables(feature='unavailable_strings', poem_flag=True, df=poem_exclude_not_en.copy())\n",
    "# 'copyright'Ïù¥ÎùºÎäî Îã®Ïñ¥Í∞Ä Ïã§Ï†ú ÏãúÏùò Î¨∏Ïû•Ïóê Ïì∞Ïù∏ Îã®Ïñ¥Ïù∏ Í≤ΩÏö∞ÎèÑ ÏùºÎ∂Ä ÏûàÏßÄÎßå, copyrightÏù¥ ÏãúÏóê Ïì∞Ïù∏ Í≤ΩÏö∞ÏôÄ ÏïÑÎãå Í≤ΩÏö∞Î•º ÌïòÎÇòÌïòÎÇò inpsectÌïòÍ∏∞ ÌûòÎì§Ïñ¥ÏÑú ÏùºÍ¥ÑÏ†ÅÏúºÎ°ú Î∫å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split (poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) make small corpus for faster training    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  (452413, 2)\n",
      "test set size:  (500, 2)\n"
     ]
    }
   ],
   "source": [
    "poem_final = poem_exclude_unavailables.copy()\n",
    "poem_train, poem_test, small_poem_trainset = train_test_split(poem_final.copy(), True, savedir, 'poem_train.csv', 'poem_test.csv', 'small_poem_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make corpus (text only)\n",
    "- Use lower case when saving text into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when re-loading data\n",
    "#poem_train = pd.read_csv(os.path.join(savedir, 'poem_train.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'poem_train_corpus.txt'), 'w')#entire trainset\n",
    "for i in range(poem_train.shape[0]):\n",
    "    f.write(str(poem_train.text.values[i]).lower())\n",
    "    if i != poem_train.shape[0]-1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'poem_test_corpus.txt'), 'w')\n",
    "for i in range(poem_test.shape[0]):\n",
    "    f.write(str(poem_test.text.values[i]).lower())\n",
    "    if i != poem_test.shape[0]-1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'small_poem_train_corpus.txt'), 'w')#small trainset <- used this in implementation\n",
    "for i in range(small_poem_trainset.shape[0]):\n",
    "    f.write(str(small_poem_trainset.text.values[i]).lower())\n",
    "    if i != small_poem_trainset.shape[0]-1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. exclude not-containing-language sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20362\n",
      "['1.' '1.' 'AMA.' '](' '1.' '^^^^^^^^.' '!' '2.' ':)' '!']\n",
      "(638825, 2)\n"
     ]
    }
   ],
   "source": [
    "no_words_sentence_indices = []\n",
    "for i in range(reddit.shape[0]):\n",
    "    current_string = str(reddit.text.values[i])\n",
    "    counter_dictionary = Counter(current_string)\n",
    "    num_en_str = 0\n",
    "    for en_str in en_strings:\n",
    "        if en_str in list(counter_dictionary.keys()):\n",
    "            num_en_str += counter_dictionary[en_str]\n",
    "    if num_en_str == 0 and i not in no_words_sentence_indices:\n",
    "        no_words_sentence_indices.append(i)\n",
    "print(len(no_words_sentence_indices))\n",
    "print(reddit.iloc[no_words_sentence_indices,:].sample(10).text.values)\n",
    "reddit_exclude_no_words = reddit.drop(no_words_sentence_indices, axis=0).copy()\n",
    "reddit_exclude_no_words.index = np.arange(reddit_exclude_no_words.shape[0])\n",
    "print(reddit_exclude_no_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. exclude sentences with num_words <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104770\n",
      "['i hate it.' 'They use lasers.' 'Truly miserable there.' 'Hi.'\n",
      " 'Thank you.' \"i can't sleep\" 'No waaaaay!' 'I think so!' 'Yeah.'\n",
      " 'Eat ice cream.']\n",
      "(534055, 2)\n"
     ]
    }
   ],
   "source": [
    "few_words_sentence_indices = []\n",
    "for i in range(reddit_exclude_no_words.shape[0]):\n",
    "    current_string = str(reddit_exclude_no_words.text.values[i])\n",
    "    if len(current_string.split(' ')) <= 3 and i not in few_words_sentence_indices:\n",
    "        few_words_sentence_indices.append(i)\n",
    "print(len(few_words_sentence_indices))\n",
    "print(reddit_exclude_no_words.iloc[few_words_sentence_indices,:].sample(10).text.values)\n",
    "reddit_exclude_few_words = reddit_exclude_no_words.drop(few_words_sentence_indices, axis=0).copy()\n",
    "reddit_exclude_few_words.index = np.arange(reddit_exclude_few_words.shape[0])\n",
    "print(reddit_exclude_few_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. exclude numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape:  (534055, 2)\n",
      "num_excluded:  34723\n",
      "example sentences that are excluded: \n",
      "['My 1st anniversary is this week and my wife and I are amazing.'\n",
      " 'That went from 0-100 really quickly!' \"No problem,  I'm 20 and he's 50.\"\n",
      " 'I always wake up exhausted even if I get like 10 hours of sleep'\n",
      " \"Right now it's 61F and somewhat overcast.\"\n",
      " '18-23ish were absolute shit, however.'\n",
      " \"I fucking loved The Grapes of Wrath, it's in my top 5.\"\n",
      " 'Some kids in our school live in the city (30+ minutes away.'\n",
      " '25-years-old and I still think about it.'\n",
      " 'Middle aged woman who has been here 8-ish years.']\n",
      "data shape after exclusion:  (499332, 2)\n"
     ]
    }
   ],
   "source": [
    "reddit_exclude_number = exclude_unavailables(feature='numbers', poem_flag=False, df=reddit_exclude_few_words.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. exclude marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape:  (499332, 2)\n",
      "num_excluded:  7168\n",
      "example sentences that are excluded: \n",
      "['**](/r/CasualConversation/w/etiquette)* *^Take ^a ^look ^at ^our [^subreddits ^directory.'\n",
      " \"* That might be different if you're not in the US.\"\n",
      " 'It is harder to get worthwhile books published these days (\\\\*she raises her hand\\\\*).'\n",
      " 'I seriously cried my eyes out *both times* I saw it.'\n",
      " '**](/r/CasualConversation/w/etiquette)* *^Take ^a ^look ^at ^our [^subreddits ^directory.'\n",
      " '*vigoriously scrolls down to his old imgur albums* That would be a yes.'\n",
      " 'Well, if their retort was the seriously \"what would you call black potatoes--*the n word?'\n",
      " '(Before I found out about my ADHD, I felt the *exact* same way as you.)'\n",
      " 'I do the *right* thing all the time.'\n",
      " '**](/r/CasualConversation/w/etiquette)* *^Take ^a ^look ^at ^our [^subreddits ^directory.']\n",
      "data shape after exclusion:  (492164, 2)\n"
     ]
    }
   ],
   "source": [
    "reddit_exclude_mark = exclude_unavailables(feature='marks', poem_flag=False, df=reddit_exclude_number.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. exclude not-english-sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12520\n",
      "[\"How's your evening been?\" 'Is your sleep _good_ quality sleep?'\n",
      " 'I dunno, depends on how it went?' \"And I've been gilded???\"\n",
      " \"As for illegal I haven't broken many laws.\" 'You do you, bud.'\n",
      " 'We need more people like you.' 'I always do man.'\n",
      " \"Haha it's okay if you're lazy!\"\n",
      " \"I didn't talk to anyone else almost at all.\"]\n",
      "(479644, 2)\n"
     ]
    }
   ],
   "source": [
    "not_en_indices = []\n",
    "#for i in range(reddit_exclude_mark.shape[0]):\n",
    "for i in np.arange(259680-1, reddit_exclude_mark.shape[0]):\n",
    "    current_string = str(reddit_exclude_mark.text.values[i])\n",
    "    if detect(current_string) != 'en' and i not in not_en_indices:\n",
    "        not_en_indices.append(i)\n",
    "print(len(not_en_indices))\n",
    "print(reddit_exclude_mark.iloc[not_en_indices,:].sample(10).text.values)\n",
    "reddit_exclude_not_en = reddit_exclude_mark.drop(not_en_indices, axis=0).copy()\n",
    "reddit_exclude_not_en.index = np.arange(reddit_exclude_not_en.shape[0])\n",
    "print(reddit_exclude_not_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. exclude 'copyright' and 'published by'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input df shape:  (479644, 2)\n",
      "num_excluded:  11\n",
      "example sentences that are excluded: \n",
      "['This is great, be sure to copyright this for later publication.'\n",
      " 'Copyright police, open up!'\n",
      " 'Keeping your recipe a secret is a decent plan - recipes cannot be copyrighted.'\n",
      " 'The game is the DC Comics Deck-Building Game, published by Cryptozoic Entertainment.'\n",
      " \"Maybe it's a copyright issue?\" 'Maybe book published by then?'\n",
      " 'Gets published by Jezebel with a Me Too hashtag.'\n",
      " 'Like long ass classical symphonies that are duplicated because there‚Äôs no copyright on them'\n",
      " 'The copyright holders are to blame, probably moving them to a different platform or something.'\n",
      " '[>Nintendo is notorious for calling people out for \"copyright\".']\n",
      "data shape after exclusion:  (479633, 2)\n"
     ]
    }
   ],
   "source": [
    "reddit_exclude_unavailables = exclude_unavailables(feature='unavailable_strings', poem_flag=False, df=reddit_exclude_not_en.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split (reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 479133, test: 500\n",
      "train set size:  (479133, 2)\n",
      "test set size:  (500, 2)\n"
     ]
    }
   ],
   "source": [
    "reddit_final = reddit_exclude_unavailables.copy()\n",
    "reddit_train, reddit_test, small_reddit_trainset = train_test_split(reddit_final.copy(), True, savedir, 'reddit_train.csv', 'reddit_test.csv', 'small_reddit_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make corpus (text only)\n",
    "- Use lower case when saving text into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when re-loading data\n",
    "#reddit_train = pd.read_csv(os.path.join(savedir, 'reddit_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'reddit_train_corpus.txt'), 'w')#entire trainset\n",
    "for i in range(reddit_train.shape[0]):\n",
    "    f.write(str(reddit_train.text.values[i]).lower())\n",
    "    if i != reddit_train.shape[0]-1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'reddit_test_corpus.txt'), 'w')\n",
    "for i in range(reddit_test.shape[0]):\n",
    "    f.write(str(reddit_test.text.values[i]).lower())\n",
    "    if i != reddit_test.shape[0]-1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'small_reddit_train_corpus.txt'), 'w')#small trainset <- used this in implementation\n",
    "for i in range(small_reddit_trainset.shape[0]):\n",
    "    f.write(str(small_reddit_trainset.text.values[i]).lower())\n",
    "    if i != small_reddit_trainset.shape[0]-1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'entire_train_corpus.txt'), 'w')\n",
    "for i in range(poem_train.shape[0]):\n",
    "    f.write(str(poem_train.text.values[i]).lower())\n",
    "    f.write(\"\\n\")\n",
    "for k in range(reddit_train.shape[0]):\n",
    "    f.write(str(reddit_train.text.values[k]).lower())\n",
    "    if k != reddit_train.shape[0] -1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(savedir, 'entire_small_train_corpus.txt'), 'w')\n",
    "for i in range(small_poem_trainset.shape[0]):\n",
    "    f.write(str(small_poem_trainset.text.values[i]).lower())\n",
    "    f.write(\"\\n\")\n",
    "for k in range(small_reddit_trainset.shape[0]):\n",
    "    f.write(str(small_reddit_trainset.text.values[k]).lower())\n",
    "    if k != small_reddit_trainset.shape[0] -1:\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
